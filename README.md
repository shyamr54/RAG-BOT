# Simple RAG System with LangChain + LangGraph

## What is this?
A basic Retrieval-Augmented Generation (RAG) pipeline that:
- Loads a small set of documents from a `documents` folder.
- Retrieves relevant information using semantic search (FAISS + HuggingFace).
- Answers user questions using a Language Model (powered by OpenRouter or a mock LLM).
- Orchestrates the entire logic using LangGraph.

## How to Run

### 1. Create a `.env` file
In the root of the project, create a new file named `.env` and add your OpenRouter API key to it:

```env
OPENROUTER_API_KEY="your_real_api_key_here"
```

If you don't have an API key, the script will run in a demo mode with a mock LLM.

### 2. Install Dependencies
Install all the required Python libraries using the `requirements.txt` file:
```sh
pip install -r requirements.txt
```

### 3. Run the Script
Execute the main Python script to start the RAG system:
```sh
python tp.py
```

The script will first run a few demo questions and then enter an interactive chat mode where you can ask your own questions.

## What I Learned / Challenges
-Secure API Key Management: Using a `.env` file is a much better practice for handling sensitive keys than hardcoding them.

-Dependency Management: A `requirements.txt` file is essential for making a project easily reproducible.
-Cross-Platform Compatibility: Using `os.path.basename()` instead of string splitting on `/` makes file path manipulation more robust across different operating systems (like Windows).

-LangGraph Orchestration: LangGraph provides a clear and powerful way to define and visualize the flow of logic in a multi-step AI system.

-Mocking for Development: Creating a simple mock LLM allows for rapid testing and development without relying on (and paying for) external API calls.

## Sample Q&A
See `sample_runs.txt` for example questions and answers generated by the system. 